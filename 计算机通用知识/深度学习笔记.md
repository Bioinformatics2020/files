# 深入理解机器学习-从原理到算法

Shai Shalev-Shwartz；Shai Ben-David著，张文生译(2016年)

### 第一章 引论

**1.1 什么是学习**

- 对经验归纳整理
- 先验知识越强越容易学习，同时学习越不灵活

**1.2 什么时候需要机器学习**

- 过于复杂或难以良好定义的问题，只能从经验中学习
- 数据量过大，超出人类理解能力的问题
- 需要随环境变化而自适应的问题

**1.3 学习的种类**

监督与无监督：

​	监督：训练数据包含标签，测试数据要求识别出标签信息

​	无监督：训练数据无额外信息，要求将数据浓缩与聚类

​	强化学习：训练数据不包含标签，但是对结果打分

主动学习与被动学习：

​	被动学习：等待用户传入训练数据

​	主动学习：用户挑选高价值的数据进行训练

**1.4 与其它领域的关系**

人工智能或计算机科学的分支，与统计学、信息论、博弈论、最优化等数学分支紧密结合

## 第一部分 理论基础

### 第二章 简易入门

**2.1 一般模型--统计学习理论框架**

1. 学习器的输入：领域集、标签集、训练数据
2. 学习器的输出：预测器或假设或分类器
3. 数据采集：按任意概率分布采集数据，并对其添加标签，一部分作为训练数据，另一部分作为测试数据
4. 衡量成果：预测正确的概率或预测误差值

**2.2 经验误差最小化**

选择合适的分类器函数，使训练数据的训练误差或经验误差或经验风险最小化(ERM)

可能出现过拟合

经验误差：模型在训练集上的误差

泛化误差：模型在新样本集（测试集）上的误差

**2.3 考虑归纳偏置的经验风险最小化**

使用归纳偏置方法，对模型做一些约束，使其优先考虑某些属性，用来减少过拟合风险。

使用什么假设类或优先考虑什么属性防止过拟合时效果更好？

### 第三章 一般学习模型

一般学习模型--可能近似正确(PAC)学习模型及其延伸

**3.1 PAC学习理论**

同等条件下，模型越复杂泛化误差越大；同一个模型，样本数量越大泛化误差越小。

模型越复杂则越需要更多的样本。

# 深度学习读书笔记

### 第一章 前言

机器学习的知识库方法：力求将关于世界的知识通过公式保存下来，由计算机基于这些知识使用逻辑回归等方法来推理学习。

逻辑回归：基于给定的训练样本特征，预测测试样本属于某一类别的概率。

传统机器学习的问题：难以提取样本特征。

表示学习：使用机器学习来发掘样本表示特征，这种方法比人工设计特征简单高效。

变差因素：可以对样本进行区分的因素

自编码器：使用编码器将数据转换为另一种形式的表示特征，使用解码器将表示特征转换回近似的原始数据

深度学习：原始数据的表示特征难以直接被计算机利用，需要多次编码器计算来识别特征，将复杂特征提取为抽象的简单特征。

深度学习属于表示学习，表示学习属于机器学习，机器学习属于AI

深度学习流程：数据输入--简单特征采样--抽象特征提取--特征与数据建立映射关系--数据输出

强化学习：在没有人类指导的情况下，通过试错来学习

### 第二章 线性代数

标量(scalar)，向量(vector)，矩阵(matrix)

张量(tensor)：n维数组

转置(transpose)：矩阵按照对主角线的镜像

矩阵乘积(matrix product)：$C_{i,j}=\sum\limits_{k} A_{i,k}B_{k,j}$

单位矩阵(identity matrix)：主对角线的元素是1，其它元素是0

矩阵的逆(matrix inverse)：矩阵的逆 乘 矩阵等于单位矩阵

线性相关(linear dependence)：一组向量的任意一个向量不能表示成其它向量的线性组合

生成子空间(span)：矩阵向量线性组合表示的值域(range)

范数(norm)：衡量向量或矩阵大小的数，1范数表示向量各维度绝对值之和，2范数表示向量各维度平方和再开方，等同于向量长度

- L1范数：$\|\mathbf{x} \|_1 = \sum_{i=1}^{n} |x_i|$
- L2范数（欧几里得范数）：$\|\mathbf{x} \|_2 = \sqrt{\sum_{i=1}^{n} |x_i|^2}$，表示向量长度
- Frobenius范数：$\|\mathbf{A} \|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}$，类似于向量的L2范数，表示矩阵大小

对角矩阵(diagonal matrix)：主对角线上的元素非0，其它元素为0

正交矩阵(orthogonal matrix)：满足$\mathbf{A}^\top \mathbf{A} = \mathbf{A} \mathbf{A}^\top = \mathbf{I}_n$条件的矩阵，性质： $\mathbf{A}^{-1} = \mathbf{A}^\top$

特征分解(eigendecomposition)：将矩阵分解为一组特征向量和特征值

特征向量(eigenvector)与特征值(eigenvalue)：矩阵A与向量相乘后，向量不改变方向那么这个向量就是矩阵的特征向量，向量缩放的比例就是对应的特征值。例如：$A v = \lambda v$，其中v表示特征向量，lambda表示特征值