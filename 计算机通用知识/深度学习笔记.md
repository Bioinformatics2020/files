# 深入理解机器学习-从原理到算法

Shai Shalev-Shwartz；Shai Ben-David著，张文生译(2016年)

### 第一章 引论

**1.1 什么是学习**

- 对经验归纳整理
- 先验知识越强越容易学习，同时学习越不灵活

**1.2 什么时候需要机器学习**

- 过于复杂或难以良好定义的问题，只能从经验中学习
- 数据量过大，超出人类理解能力的问题
- 需要随环境变化而自适应的问题

**1.3 学习的种类**

监督与无监督：

​    监督：训练数据包含标签，测试数据要求识别出标签信息

​    无监督：训练数据无额外信息，要求将数据浓缩与聚类

​    强化学习：训练数据不包含标签，但是对结果打分

主动学习与被动学习：

​    被动学习：等待用户传入训练数据

​    主动学习：用户挑选高价值的数据进行训练

**1.4 与其它领域的关系**

人工智能或计算机科学的分支，与统计学、信息论、博弈论、最优化等数学分支紧密结合

## 第一部分 理论基础

### 第二章 简易入门

**2.1 一般模型--统计学习理论框架**

1. 学习器的输入：领域集、标签集、训练数据
2. 学习器的输出：预测器或假设或分类器
3. 数据采集：按任意概率分布采集数据，并对其添加标签，一部分作为训练数据，另一部分作为测试数据
4. 衡量成果：预测正确的概率或预测误差值

**2.2 经验误差最小化**

选择合适的分类器函数，使训练数据的训练误差或经验误差或经验风险最小化(ERM)

可能出现过拟合

经验误差：模型在训练集上的误差

泛化误差：模型在新样本集（测试集）上的误差

**2.3 考虑归纳偏置的经验风险最小化**

使用归纳偏置方法，对模型做一些约束，使其优先考虑某些属性，用来减少过拟合风险。

使用什么假设类或优先考虑什么属性防止过拟合时效果更好？

### 第三章 一般学习模型

一般学习模型--可能近似正确(PAC)学习模型及其延伸

**3.1 PAC学习理论**

同等条件下，模型越复杂泛化误差越大；同一个模型，样本数量越大泛化误差越小。

模型越复杂则越需要更多的样本。

# 深度学习读书笔记

### 第一章 前言

机器学习的知识库方法：力求将关于世界的知识通过公式保存下来，由计算机基于这些知识使用逻辑回归等方法来推理学习。

逻辑回归：基于给定的训练样本特征，预测测试样本属于某一类别的概率。

传统机器学习的问题：难以提取样本特征。

表示学习：使用机器学习来发掘样本表示特征，这种方法比人工设计特征简单高效。

变差因素：可以对样本进行区分的因素

自编码器：使用编码器将数据转换为另一种形式的表示特征，使用解码器将表示特征转换回近似的原始数据

深度学习：原始数据的表示特征难以直接被计算机利用，需要多次编码器计算来识别特征，将复杂特征提取为抽象的简单特征。

深度学习属于表示学习，表示学习属于机器学习，机器学习属于AI

深度学习流程：数据输入--简单特征采样--抽象特征提取--特征与数据建立映射关系--数据输出

强化学习：在没有人类指导的情况下，通过试错来学习

### 第二章 线性代数

标量(scalar)，向量(vector)，矩阵(matrix)

张量(tensor)：n维数组

转置(transpose)：矩阵按照对主角线的镜像

矩阵乘积(matrix product)：$C_{i,j}=\sum\limits_{k} A_{i,k}B_{k,j}$

单位矩阵(identity matrix)：主对角线的元素是1，其它元素是0

矩阵的逆(matrix inverse)：矩阵的逆 乘 矩阵等于单位矩阵

线性相关(linear dependence)：一组向量的任意一个向量不能表示成其它向量的线性组合

生成子空间(span)：矩阵向量线性组合表示的值域(range)

范数(norm)：衡量向量或矩阵大小的数，1范数表示向量各维度绝对值之和，2范数表示向量各维度平方和再开方，等同于向量长度

- L1范数：$\|\mathbf{x} \|_1 = \sum_{i=1}^{n} |x_i|$
- L2范数（欧几里得范数）：$\|\mathbf{x} \|_2 = \sqrt{\sum_{i=1}^{n} |x_i|^2}$，表示向量长度
- Frobenius范数：$\|\mathbf{A} \|_F = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} |a_{ij}|^2}$，类似于向量的L2范数，表示矩阵大小
- 无穷范数：最大或最小绝对值

对角矩阵(diagonal matrix)：主对角线上的元素非0，其它元素为0

正交矩阵(orthogonal matrix)：满足$\mathbf{A}^\top \mathbf{A} = \mathbf{A} \mathbf{A}^\top = \mathbf{I}_n$条件的矩阵，性质： $\mathbf{A}^{-1} = \mathbf{A}^\top$

特征分解(eigendecomposition)：将矩阵分解为一组特征向量和特征值，根据特征向量属性，可以推导出对应的特征值与特征向量

特征向量(eigenvector)与特征值(eigenvalue)：矩阵A与向量相乘后，向量不改变方向那么这个向量就是矩阵的特征向量，向量缩放的比例就是对应的特征值。例如：$A v = \lambda v$，其中v表示特征向量，lambda表示特征值

单位特征向量：L2范数为1的特征向量

正定矩阵(positive definite)：所有特征值都是正数的矩阵就是正定矩阵

行列式：特征值的乘机，可以表示矩阵空间扩大或缩小的比例

主成分分析：用尽可能少的数据，描绘原始数据的特征，分为数据的编码与解码两步

## 第三章 概率与信息论

不确定性的来源：系统内在的随机、不完全观测、不完全建模。很多情况下简单而不完全的建模比复杂而较完全的建模更实用。

频率概率：根据试验现象出现的频率来统计的概率

贝叶斯概率：根据理论计算得出的确定的概率

概率分布：随机变量取到某个状态的可能性统计

b满足的前提下，a满足的概率：$ P(A|B) = \frac{P(A \cap B)}{P(B)} $

a,b同时满足的概率：$P(a,b)=P(a|b)P(b)$

函数f(x)关于某分布P(x)的期望(expectation)：$ E _{x \thicksim p}[f(x)] = \int f(x) \cdot p(x) \, dx $

### 第四章 数值计算

条件数：函数相对于输入的微小变化而变化的快慢程度

偏导数：$ \frac{\partial}{\partial x} f(x) $ 沿某一个自变量方向变化的导数

梯度：$ \nabla_xf(x) $  所有自变量偏导数的集合

# 其它

神经网络的基本要素：模型、训练数据、损失函数、优化算法